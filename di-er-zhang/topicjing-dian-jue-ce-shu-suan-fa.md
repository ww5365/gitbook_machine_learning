# topic-经典决策树算法

## 一、熵

随机变量X，具有不确定性，怎么量化的描述这种不确定性？

* 信息熵：

假设X的概率分布情况：X={p1,p2,...pn}

![](/assets/2-juceshusuanfa-1.png)

熵越大，X的不确定性越大，确定X需要的额外信息就越多，本质的信息量就越少；

ps: 
$$f(x) = -xlogx$$

![](/assets/2-xlogx.png)

这个函数有哪些特征？为什么选择这个函数？
1、(0,1)之间：中间大，两头小；
2、对于二分类，两种概率分布情况{p,1-p},如果发生的概率很大或很小，发现信息熵H(x)小，也就是不确定性就小，本质的信息量大；
3、发生的概率越偏向中间，不确定性就越大，熵越大，本质的信息量小；
4、选ta还是明智的。

* 条件熵：

我们的X在知道Y后，还剩下多少不确定性？使用条件熵来描述

![](/assets/2-juceshusuanfa-2.png)

* 信息增益：

g\(X,Y\) = H\(X\) - H\(X/Y\)

理解：

对于待划分的数据集D，其 entroy\(前\)是一定的，但是划分之后的熵 entroy\(后\)是不定的，entroy\(后\)越小说明使用此特征划分得到的子集的不确定性越小（也就是纯度越高），  
 因此 **entroy\(前\) -  entroy\(后\)差异越大，说明使用当前特征划分数据集D的话，其纯度上升的更快**。而我们在构建最优的决策树的时候总希望能更快速到达纯度更高的集合，这一点可以参考优化算法中的梯度下降算法，每一步沿着负梯度方向最小化损失函数的原因就是负梯度方向是函数值减小最快的方向。同理：在决策树构建的过程中我们总是希望集合往最快到达纯度更高的子集合方向发展，因此我们总是选择使得信息增益最大的特征来划分当前数据集D。

* 信息增益率

![](/assets/2-juceshusuanfa-4.png)

$$ H_A(D) $$: 把特征A作为分类标准，计算其信息熵，作为一个惩罚因子；

解决的问题？
仅使用信息增益，存在偏向样本特征中取值较多的特征；也就是说特征样本多，被选中的可能性高；有偏向；所以引入增益率；


## 二 决策树

### 2.1 常见决策树算法

| 算法 | 特征选择 |
| :--- | :--- |
| ID3 | 信息增益 |
| C4.5 | 信息增益率 |
| CART | 分类树：基尼指数   回归树：最小二乘 ？ |


使用自己的语言描述：ID3 决策树构建算法？

1、对于训练集合D，计算每个特征的信息增益；选择信息增益最大(此特征区分度强，便于提纯数据)的特征A0作为当前决策节点；
2、对D-D(A0)后的集合，重复1，继续构建决策树；


### 2.2 实例计算

计算下面训练实例的信息增益和信息增益率？基尼指数？

![](/assets/2-jueceshusuanfa-3.png)

集合D整体熵：
H\(D\)=?
某种特种条件下的熵：
H\(D,A1\) = ?
信息增益=？
信息增益率=?  

![](/assets/2-jueceshusuanfa-7.jpg)



## 三 CART 算法详解


classification and regression tree: 也就是说是分类树，也是回归树


### 3.1 分类树

决策条件： 基尼指数

$$ Gini(p) = \sum_{k=1}^K(p_k(1-p_k)) $$ = $$ 1- \sum_{k=1}^K(p_k)^2 $$

假设分类为K，样本点属于某类k的概率为$$p_k$$，则概率分布$$\{p_1,p_2,...p_k\}$$的基尼指数计算如上面的式子；

基尼指数越大，说明样本分布再各个类别中，数据集合D的混乱程度就越高些，不容易区分确认分类；如果基尼指数越小，说明样本集中分布在某个类别中，区分度越大些；


集合D的基尼指数定义：

按照特征A=a中某个值，集合被分成D1，D2两个部分： $$D_1 = \{(x,y)\in D| A(x) = a\}, D_2 = D - D_1$$

集合D的基尼指数：

$$Gini(D,A) = \frac {|D_1|}{|D|}gini(D_1) + \frac {|D_2|}{|D|}gini(D_2)$$

标识含义：集合D经过特征A=a的分割后，集合D的不确定性；

ps：
1、基尼指数是$$x-x^2$$函数，最值在1/2附近；
2、基尼指数越大，说明分布越分散；如果发生的概率少或很大(可以认为数据越纯)，基尼指数其实会变小；
3、实际也是选择了：中间大两头小函数


利用基尼指数构造决策树的实例和算法：？





### 3.2 回归树

个人直觉上理解，就是用分段函数来拟合数据。怎么把数据一步步切分开，之后来逐步拟合数据？

假设一堆的(x,y)数据，穷举方法，来寻找切分点s；对每个可能切分点，分别计算每个集合的y真实值和y的均值之差的平方；最终选择？选择均方差最小的切分点s；


最小二乘的算法：？

 (1):穷举，划分集合；计算均值和方差和
集合：$$ R1 = \{x|x < s\}  R2 = \{x|x>s\} $$

$$c1 = \frac{1}{N}\sum{y_i} \text{,(x,y)是属于集合R1}  $$
$$c2 = \frac{1}{M}\sum{y_i} \text{,(x,y)是属于集合R2}  $$

$$ \{\sum_{i\in R1}(y_i-c_1)^2 + \sum_{j\in R2}(y_j-c_2)^2\}$$


(2):选取一轮中，差方和最小的点s，作为切分点；
$$min\{\sum_{i\in R1}(y_i-c_1)^2 + \sum_{j\in R2}(y_j-c_2)^2\}$$

需要将c1，c2加入到结果集合$$c_m$$,对应的特征域:$$R_m$$

(3):下一轮(>=2),重复1，2

(4): $$f(x) = \sum_{m=1}^{M}c_mI (x \in R_m)$$



### 3.3 回归树实例




* 算法实现说明：

 1、具体实例可见参考4；第二轮之后使用残差进行计算(也就是gradient boosting 的思想)

 2、实例实现的疑问：？
   (1) 第二轮后，为什么使用残差来切分和拟合数据？算法精华所在。。

   首先，使用残差是可以的。
   f1= T1
   f2 = f1 + T2;
   Tx 是本轮切分点，y均值

   最终目标是达到残差最小；

   其次，方便计算真实值分段函数；
   如果不实用残差，后面只能得到切分点，不容易得到分段函数的值；通过使用残差，迭代每一轮





### 3.4 回归树实现代码

https://github.com/ww5365/my-gbrank/regression_tree.py

代码实现tips：

* 对样本会先按照X的特征值进行排序，按照排序后的顺序寻找切分点。

* 可以存在问题？使用特征<0和>=0判断,有问题。


![](/assets/2-juceshusuanfa-6.png)














 
  
    

## 

## 参考

1、[https://juejin.im/post/5a16bd40f265da430c117d64](https://juejin.im/post/5a16bd40f265da430c117d64)  
《经典决策树算法》

2、[https://www.cnblogs.com/muzixi/p/6566803.html](https://www.cnblogs.com/muzixi/p/6566803.html)


3、http://shiyanjun.cn/archives/428.html

everynote笔记：《分类算法：决策树（C4.5）》
有计算决策树c4.5的详细过程演示


3、https://juejin.im/post/5a16b2276fb9a044fe4608c8
CART--诞生即有理

4、https://cethik.vip/2016/09/21/machineCAST/
这篇文章实例说明回归树的建立，利用了残差计算；






