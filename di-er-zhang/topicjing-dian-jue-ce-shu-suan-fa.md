# topic-经典决策树算法

## 一、熵

随机变量X，具有不确定性，怎么量化的描述这种不确定性？

* 信息熵：

假设X的概率分布情况：X={p1,p2,...pn}

![](/assets/2-juceshusuanfa-1.png)

熵越大，X的不确定性越大，确定X需要的额外信息就越多，本质的信息量就越少；

* 条件熵：

我们的X在知道Y后，还剩下多少不确定性？使用条件熵来描述

![](/assets/2-juceshusuanfa-2.png)

* 信息增益：

g\(X,Y\) = H\(X\) - H\(X/Y\)

理解：

对于待划分的数据集D，其 entroy\(前\)是一定的，但是划分之后的熵 entroy\(后\)是不定的，entroy\(后\)越小说明使用此特征划分得到的子集的不确定性越小（也就是纯度越高），  
 因此 **entroy\(前\) -  entroy\(后\)差异越大，说明使用当前特征划分数据集D的话，其纯度上升的更快**。而我们在构建最优的决策树的时候总希望能更快速到达纯度更高的集合，这一点可以参考优化算法中的梯度下降算法，每一步沿着负梯度方法最小化损失函数的原因就是负梯度方向是函数值减小最快的方向。同理：在决策树构建的过程中我们总是希望集合往最快到达纯度更高的子集合方向发展，因此我们总是选择使得信息增益最大的特征来划分当前数据集D。

* 信息增益率

![](/assets/2-juceshusuanfa-4.png)

H$$A$$\(D\): 把特征A作为随机变量，计算其信息熵，作为一个惩罚因子；

* 计算下面训练实例的信息增益和信息增益率？

![](/assets/2-jueceshusuanfa-3.png)

H\(D\)=?

H\(D,A1\) = ?







## 二 决策树算法总结

| 算法 | 特征选择 |
| :--- | :--- |
| ID3 | 信息增益 |
| C4.5 | 信息增益率 |
| CART | 分类树：基尼指数   回归树：最小二乘 ？ |


使用自己的语言描述：ID3 决策树构建算法？

1、对于训练集合D，计算每个特征的信息增益；选择信息增益最大(此特征区分度强，便于提纯数据)的特征A0作为当前决策节点；
2、对D-D(A0)后的集合，重复1，继续构建决策树；



## 三 CART


classification and regression tree: 也就是说是分类树，也是回归树


### 分类树

决策条件： 基尼指数

Gini(p) = $$ \sum_{k=1}^K(p_k(1-p_k)) $$


$$ J_\alpha(x) = \sum_{m=0}^\infty \frac{(-1)^m}{m! \Gamma (m + \alpha + 1)} {\left({ \frac{x}{2} }\right)}^{2m + \alpha} \text {，独立公式示例} $$




 
  
    

## 

## 参考

1、[https://juejin.im/post/5a16bd40f265da430c117d64](https://juejin.im/post/5a16bd40f265da430c117d64)  
《经典决策树算法》

2、[https://www.cnblogs.com/muzixi/p/6566803.html](https://www.cnblogs.com/muzixi/p/6566803.html)


3、https://juejin.im/post/5a16b2276fb9a044fe4608c8
CART--诞生即有理



