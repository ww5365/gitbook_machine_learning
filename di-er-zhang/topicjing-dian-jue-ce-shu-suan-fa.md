# topic-经典决策树算法

## 一、熵

随机变量X，具有不确定性，怎么量化的描述这种不确定性？

* 信息熵：

假设X的概率分布情况：X={p1,p2,...pn}

![](/assets/2-juceshusuanfa-1.png)

熵越大，X的不确定性越大，确定X需要的额外信息就越多，本质的信息量就越少；

* 条件熵：

我们的X在知道Y后，还剩下多少不确定性？使用条件熵来描述

![](/assets/2-juceshusuanfa-2.png)

* 信息增益：

g\(X,Y\) = H\(X\) - H\(X/Y\)

理解：

对于待划分的数据集D，其 entroy\(前\)是一定的，但是划分之后的熵 entroy\(后\)是不定的，entroy\(后\)越小说明使用此特征划分得到的子集的不确定性越小（也就是纯度越高），  
 因此 **entroy\(前\) -  entroy\(后\)差异越大，说明使用当前特征划分数据集D的话，其纯度上升的更快**。而我们在构建最优的决策树的时候总希望能更快速到达纯度更高的集合，这一点可以参考优化算法中的梯度下降算法，每一步沿着负梯度方法最小化损失函数的原因就是负梯度方向是函数值减小最快的方向。同理：在决策树构建的过程中我们总是希望集合往最快到达纯度更高的子集合方向发展，因此我们总是选择使得信息增益最大的特征来划分当前数据集D。

* 信息增益率

![](/assets/2-juceshusuanfa-4.png)

$$ H_A(D) $$: 把特征A作为随机变量，计算其信息熵，作为一个惩罚因子；

表示什么含义？




* 计算下面训练实例的信息增益和信息增益率？基尼指数？

![](/assets/2-jueceshusuanfa-3.png)

H\(D\)=?

H\(D,A1\) = ?







## 二 决策树算法总结

| 算法 | 特征选择 |
| :--- | :--- |
| ID3 | 信息增益 |
| C4.5 | 信息增益率 |
| CART | 分类树：基尼指数   回归树：最小二乘 ？ |


使用自己的语言描述：ID3 决策树构建算法？

1、对于训练集合D，计算每个特征的信息增益；选择信息增益最大(此特征区分度强，便于提纯数据)的特征A0作为当前决策节点；
2、对D-D(A0)后的集合，重复1，继续构建决策树；



## 三 CART


classification and regression tree: 也就是说是分类树，也是回归树


### 3.1 分类树

决策条件： 基尼指数

$$ Gini(p) = \sum_{k=1}^K(p_k(1-p_k)) $$ = $$ 1- \sum_{k=1}^K(p_k)^2 $$

假设分类为K，样本点属于某类k的概率为$$p_k$$，则概率分布$$\{p_1,p_2,...p_k\}$$的基尼指数计算如上面的式子；

基尼指数越大，说明样本分布再各个类别中，数据集合D的混乱程度就越高些，不容易区分确认分类；如果基尼指数越小，说明样本集中分布在某个类别中，区分度越大些；


集合D的基尼指数定义：

按照特征A=a中某个值，集合被分成D1，D2两个部分： $$D_1 = \{(x,y)\in D| A(x) = a\}, D_2 = D - D_1$$

集合D的基尼指数：

$$Gini(D,A) = \frac {|D_1|}{|D|}gini(D_1) + \frac {|D_2|}{|D|}gini(D_2)$$

标识含义：集合D经过特征A=a的分割后，集合D的不确定性；


利用基尼指数构造决策树的实例和算法：？





### 3.2 回归树

直觉上理解，就是用分段函数来拟合数据。


疑问点：

使用残差值，来计算切分点s，可以吗？(两边的残差是减去不同值得到的）




 
  
    

## 

## 参考

1、[https://juejin.im/post/5a16bd40f265da430c117d64](https://juejin.im/post/5a16bd40f265da430c117d64)  
《经典决策树算法》

2、[https://www.cnblogs.com/muzixi/p/6566803.html](https://www.cnblogs.com/muzixi/p/6566803.html)

3、http://shiyanjun.cn/archives/428.html

有计算决策树c4.5的详细过程


3、https://juejin.im/post/5a16b2276fb9a044fe4608c8
CART--诞生即有理

这篇文章实例说明回归树的建立，参考：https://cethik.vip/2016/09/21/machineCAST/





