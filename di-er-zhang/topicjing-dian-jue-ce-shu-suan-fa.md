# topic-经典决策树算法

## 一、熵

随机变量X，具有不确定性，怎么量化的描述这种不确定性？

* 信息熵：

假设X的概率分布情况：X={p1,p2,...pn}

![](/assets/2-juceshusuanfa-1.png)

熵越大，X的不确定性越大，确定X需要的额外信息就越多，本质的信息量就越少；

* 条件熵：

我们的X在知道Y后，还剩下多少不确定性？使用条件熵来描述

![](/assets/2-juceshusuanfa-2.png)

* 信息增益：

g\(X,Y\) = H\(X\) - H\(X/Y\)

理解：

对于待划分的数据集D，其 entroy\(前\)是一定的，但是划分之后的熵 entroy\(后\)是不定的，entroy\(后\)越小说明使用此特征划分得到的子集的不确定性越小（也就是纯度越高），  
 因此 **entroy\(前\) -  entroy\(后\)差异越大，说明使用当前特征划分数据集D的话，其纯度上升的更快**。而我们在构建最优的决策树的时候总希望能更快速到达纯度更高的集合，这一点可以参考优化算法中的梯度下降算法，每一步沿着负梯度方法最小化损失函数的原因就是负梯度方向是函数值减小最快的方向。同理：在决策树构建的过程中我们总是希望集合往最快到达纯度更高的子集合方向发展，因此我们总是选择使得信息增益最大的特征来划分当前数据集D。

* 信息增益率

![](/assets/2-juceshusuanfa-4.png)

H$$A$$\(D\): 把特征A作为随机变量，计算其信息熵，作为一个惩罚因子；





* 计算下面

## 二 决策树算法

| 算法 | 特征选择 |
| :--- | :--- |
| ID3 | 信息增益 |
|  |  |
|  |  |

## 

## 

## 参考

1、[https://juejin.im/post/5a16bd40f265da430c117d64](https://juejin.im/post/5a16bd40f265da430c117d64)  
《经典决策树算法》

2、[https://www.cnblogs.com/muzixi/p/6566803.html](https://www.cnblogs.com/muzixi/p/6566803.html)

